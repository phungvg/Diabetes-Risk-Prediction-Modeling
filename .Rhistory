axis.line = element_line(color = "black"),
axis.ticks = element_line(color = "black"),
plot.title = element_text(face = "bold", color = "#333333"),
axis.title = element_text(color = "#333333"),
axis.text = element_text(color = "#333333")
) +
labs(
title = paste("Distribution of", var),
x = "Value",
y = "Count"
)
print(p)
}
# Data split
set.seed(123)
#Split train =75,test=25
train_index <- createDataPartition(data$Diabetes, p = 0.7, list = FALSE)
train_raw <- data[train_index, ]
test_raw  <- data[-train_index, ]
# Separate predictors and outcome for scaling
x_train_raw <- train_raw %>% select(-Diabetes)
x_test_raw  <- test_raw  %>% select(-Diabetes)
# Preprocess predictors for methods that need scaling (SVM and KNN)
# We center and scale all predictors
preproc <- preProcess(x_train_raw, method = c("center", "scale"))
x_train_scaled <- predict(preproc, x_train_raw)
x_test_scaled  <- predict(preproc, x_test_raw)
# Create scaled data frames with the same outcome as the raw ones
train_scaled <- cbind(x_train_scaled, Diabetes = train_raw$Diabetes)
test_scaled  <- cbind(x_test_scaled,  Diabetes = test_raw$Diabetes)
#pre_class: Yes,No
#Prob_pos: Numeric vector of P(yes) or null if none
#truth: true factor with levels yes or no
#model type: choose in 5
get_metrics <- function(pred_class, prob_positive, truth, model_name) {
# pred_class: factor predictions with levels "No","Yes"
# prob_positive: numeric vector of P(Yes) or NULL if not available
# truth: true factor with levels "No","Yes"
cat("\n==============================\n")
cat(model_name, "\n")
cat("==============================\n")
cm <- confusionMatrix(pred_class, truth, positive = "Yes")
print(cm$table)
cat("\nAccuracy:", cm$overall["Accuracy"], "\n")
cat("Sensitivity:", cm$byClass["Sensitivity"], "\n")
cat("Specificity:", cm$byClass["Specificity"], "\n")
cat("Precision:",  cm$byClass["Precision"],  "\n")
cat("F1:",         cm$byClass["F1"],         "\n")
if (!is.null(prob_positive)) {
roc_obj <- roc(
response = truth,
predictor = prob_positive,
levels = c("No", "Yes")
)
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")
} else {
roc_obj <- NA
auc_value <- NA
cat("AUC not available because probabilities not supplied\n")
}
list(
cm = cm,
roc = roc_obj,
auc = as.numeric(auc_value)
)
}
logistic_regression_model <- glm(Diabetes ~.,data = train_raw,family = binomial)
svm_model <- svm(Diabetes ~., data = train_scaled, kernal ='radial', cost=1, probability =TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Let k pick themselves by looping to test multiple values and pick the best one
set.seed(123)
#Test odd k from 3 to 21 to pick the best one
knn_grid <- expand.grid(k=seq(3,21,by=2))
knn_model <- train(Diabetes ~ .,
data = train_raw,
method = "knn",
trControl = train_control,
preProcess = c("center", "scale"), # Critical for KNN
tuneGrid = knn_grid,
metric = "ROC")
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/panda/Documents/School/UIC/Semester/Now/Fall 2025/BSTT_527/Final Project/")
library(readr)
library(dplyr)
library(caret)
library(e1071)  #  SVM
library(rpart)  #  Decision Tree
library(randomForest)
library(pROC)   # ROC,AUC
# library(tidyverse)
library(rpart.plot)
# library(class)
library(tree)
library(ggplot2)
# head(data,5)
# summary(data)
# Sample size by outcome
table(data$Diabetes)
# Or more descriptive
cat("Yes (Diabetes): n =", sum(data$Diabetes == 1), "\n")
cat("No (Diabetes): n =", sum(data$Diabetes == 0), "\n")
data$Diabetes <- factor(data$Diabetes,
levels= c(0,1),
labels =c("No","Yes"))
# data$Diabetes
tbl1 <- table(data$Diabetes)
tbl1
prop.table(table(data$Diabetes))
#Main numerical variables
numerical_var <- c("Age","BMI", "GenHlth", "MentHlth", "PhysHlth")
main <- data %>%
summarise(
across(
all_of(numerical_var),
list(
mean  = ~mean(., na.rm = TRUE),
median = ~median(., na.rm = TRUE),
sd     = ~sd(., na.rm = TRUE),
min    = ~min(., na.rm = TRUE),
max    = ~max(., na.rm = TRUE)
),
.names = "{.col}_{.fn}"
)
)
print("Main statistics for key numeric variables")
main
# data %>%
#  pivot_longer(all_of(numerical_var)) %>%
#  ggplot(aes(x = value)) +
#  geom_histogram(bins = 30, color = "white") +
#  facet_wrap(~ name, scales = "free") +
#  theme_minimal() +
#  labs(
#    title = "Distribution of numeric predictors",
#    x = "Value",
#    y = "Count"
#  )
# Loop through each numeric variable and plot individually
# Define custom colors for each variable
custom_colors <- c("Age" = "#1f77b4", "BMI" = "#ff7f0e", "GenHlth" = "#2ca02c",
"MentHlth" = "#d62728", "PhysHlth" = "#9467bd")
# Loop through each variable
for (var in numerical_var) {
# Create histogram data manually to find tallest bin
hist_data <- data %>%
select(all_of(var)) %>%
rename(value = all_of(var)) %>%
mutate(bin = cut(value, breaks = 30)) %>%
count(bin) %>%
mutate(bin_mid = as.numeric(sub("\\((.+),.*", "\\1", bin)) +
(as.numeric(sub(".*,(.+)\\]", "\\1", bin)) -
as.numeric(sub("\\((.+),.*", "\\1", bin))) / 2)
# Identify tallest bin
max_count <- max(hist_data$n)
# Plot
p <- ggplot(hist_data, aes(x = bin_mid, y = n)) +
geom_col(aes(fill = n == max_count), color = "black", width = 1) +
scale_fill_manual(values = c("TRUE" = "red", "FALSE" = custom_colors[[var]]), guide = "none") +
theme_minimal(base_size = 14) +
theme(
panel.grid = element_blank(),
axis.line = element_line(color = "black"),
axis.ticks = element_line(color = "black"),
plot.title = element_text(face = "bold", color = "#333333"),
axis.title = element_text(color = "#333333"),
axis.text = element_text(color = "#333333")
) +
labs(
title = paste("Distribution of", var),
x = "Value",
y = "Count"
)
print(p)
}
# Data split
set.seed(123)
#Split train 70,test 30
train_index <- createDataPartition(data$Diabetes, p = 0.7, list = FALSE)
train_raw <- data[train_index, ]
test_raw  <- data[-train_index, ]
# Separate predictors and outcome for scaling
# x_train_raw <- train_raw %>% select(-Diabetes)
# x_test_raw  <- test_raw  %>% select(-Diabetes)
# Preprocess predictors for methods that need scaling (SVM and KNN)
# We center and scale all predictors
# preproc <- preProcess(x_train_raw, method = c("center", "scale"))
# x_train_scaled <- predict(preproc, x_train_raw)
# x_test_scaled  <- predict(preproc, x_test_raw)
# # Create scaled data frames with the same outcome as the raw ones
# train_scaled <- cbind(x_train_scaled, Diabetes = train_raw$Diabetes)
# test_scaled  <- cbind(x_test_scaled,  Diabetes = test_raw$Diabetes)
#Trainig
x_train <- select(train_raw, -Diabetes)
y_train <- train_raw$Diabetes
#Testing
x_test <- select(test_raw, -Diabetes)
y_test <- test_raw$Diabetes
get_metrics <- function(pred_class, prob_positive, truth, model_name) {
# pred_class: factor predictions with levels "No","Yes"
# prob_positive: numeric vector of P(Yes) or NULL if not available
# truth: true factor with levels "No","Yes"
cat("\n==============================\n")
cat(model_name, "\n")
cat("==============================\n")
cm <- confusionMatrix(pred_class, truth, positive = "Yes")
print(cm$table)
cat("\nAccuracy:", cm$overall["Accuracy"], "\n")
cat("Sensitivity:", cm$byClass["Sensitivity"], "\n")
cat("Specificity:", cm$byClass["Specificity"], "\n")
cat("Precision:",  cm$byClass["Precision"],  "\n")
cat("F1:",         cm$byClass["F1"],         "\n")
if (!is.null(prob_positive)) {
roc_obj <- roc(
response = truth,
predictor = prob_positive,
levels = c("No", "Yes")
)
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")
} else {
roc_obj <- NA
auc_value <- NA
cat("AUC not available because probabilities not supplied\n")
}
list(
cm = cm,
roc = roc_obj,
auc = as.numeric(auc_value)
)
}
# 5-fold CV, returning Class Probabilities for ROC calculation
train_control <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
# Let k pick themselves by looping to test multiple values and pick the best one
set.seed(123)
#Test odd k from 3 to 21 to pick the best one
knn_grid <- expand.grid(k=seq(3,21,by=2))
knn_model <- train(Diabetes ~ .,
data = train_raw,
method = "knn",
trControl = train_control,
preProcess = c("center", "scale"), # Critical for KNN
tuneGrid = knn_grid,
metric = "ROC")
# Let k pick themselves by looping to test multiple values and pick the best one
set.seed(123)
#Test odd k from 3 to 21 to pick the best one
knn_grid <- expand.grid(k=seq(3,21,by=2))
knn_model <- train(Diabetes ~ .,
data = train_raw,
method = "knn",
trControl = train_control,
preProcess = c("center", "scale"), # Critical for KNN
tuneGrid = knn_grid,
metric = "ROC")
data <- read.csv('diabetes_data.csv', header=TRUE)
colnames(data)
str(data)
#Check NA
sum(is.na(data))
data <- read.csv('diabetes_data.csv', header=TRUE)
colnames(data)
str(data)
#Check NA
sum(is.na(data))
# Let k pick themselves by looping to test multiple values and pick the best one
set.seed(123)
#Test odd k from 3 to 21 to pick the best one
knn_grid <- expand.grid(k=seq(3,21,by=2))
knn_model <- train(Diabetes ~ .,
data = train_raw,
method = "knn",
trControl = train_control,
preProcess = c("center", "scale"), # Critical for KNN
tuneGrid = knn_grid,
metric = "ROC")
library(readr)
library(dplyr)
library(caret)
library(e1071)  #  SVM
library(rpart)  #  Decision Tree
library(randomForest)
library(pROC)   # ROC,AUC
library(rpart.plot)
library(tree)
library(ggplot2)
library(klaR)
install.packages('klaR')
library(readr)
library(dplyr)
library(caret)
library(e1071)  #  SVM
library(rpart)  #  Decision Tree
library(randomForest)
library(pROC)   # ROC,AUC
library(rpart.plot)
library(tree)
library(ggplot2)
library(klaR)
set.seed(123)
#Test odd k from 3 to 21 to pick the best one
knn_grid <- expand.grid(k=seq(3,21,by=2))
# Naive Bayes is simple and fast - no complex tuning grid needed
naives_bayes_model <- train(Diabetes ~ .,
data = train_raw,
method = "nb",
trControl = train_control,
metric = "ROC")
varImpPlot(rf_model$finalModel, main = "Random forest variable importance")
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/panda/Documents/School/UIC/Semester/Now/Fall 2025/BSTT_527/Final Project/")
library(readr)
library(dplyr)
library(caret)
library(e1071)  #  SVM
library(rpart)  #  Decision Tree
library(randomForest)
library(pROC)   # ROC,AUC
library(rpart.plot)
library(tree)
library(ggplot2)
library(klaR)
data <- read.csv('diabetes_data.csv', header=TRUE)
colnames(data)
str(data)
#Check NA
sum(is.na(data))
# head(data,5)
# summary(data)
# Sample size by outcome
table(data$Diabetes)
# Or more descriptive
cat("Yes (Diabetes): n =", sum(data$Diabetes == 1), "\n")
cat("No (Diabetes): n =", sum(data$Diabetes == 0), "\n")
data$Diabetes <- factor(data$Diabetes,
levels= c(0,1),
labels =c("No","Yes"))
# data$Diabetes
tbl1 <- table(data$Diabetes)
tbl1
prop.table(table(data$Diabetes))
#Main numerical variables
numerical_var <- c("Age","BMI", "GenHlth", "MentHlth", "PhysHlth")
main <- data %>%
summarise(
across(
all_of(numerical_var),
list(
mean  = ~mean(., na.rm = TRUE),
median = ~median(., na.rm = TRUE),
sd     = ~sd(., na.rm = TRUE),
min    = ~min(., na.rm = TRUE),
max    = ~max(., na.rm = TRUE)
),
.names = "{.col}_{.fn}"
)
)
print("Main statistics for key numeric variables")
main
# data %>%
#  pivot_longer(all_of(numerical_var)) %>%
#  ggplot(aes(x = value)) +
#  geom_histogram(bins = 30, color = "white") +
#  facet_wrap(~ name, scales = "free") +
#  theme_minimal() +
#  labs(
#    title = "Distribution of numeric predictors",
#    x = "Value",
#    y = "Count"
#  )
# Loop through each numeric variable and plot individually
# Define custom colors for each variable
custom_colors <- c("Age" = "#1f77b4", "BMI" = "#ff7f0e", "GenHlth" = "#2ca02c",
"MentHlth" = "#d62728", "PhysHlth" = "#9467bd")
# Loop through each variable
for (var in numerical_var) {
# Create histogram data manually to find tallest bin
hist_data <- data %>%
select(all_of(var)) %>%
rename(value = all_of(var)) %>%
mutate(bin = cut(value, breaks = 30)) %>%
count(bin) %>%
mutate(bin_mid = as.numeric(sub("\\((.+),.*", "\\1", bin)) +
(as.numeric(sub(".*,(.+)\\]", "\\1", bin)) -
as.numeric(sub("\\((.+),.*", "\\1", bin))) / 2)
# Identify tallest bin
max_count <- max(hist_data$n)
# Plot
p <- ggplot(hist_data, aes(x = bin_mid, y = n)) +
geom_col(aes(fill = n == max_count), color = "black", width = 1) +
scale_fill_manual(values = c("TRUE" = "red", "FALSE" = custom_colors[[var]]), guide = "none") +
theme_minimal(base_size = 14) +
theme(
panel.grid = element_blank(),
axis.line = element_line(color = "black"),
axis.ticks = element_line(color = "black"),
plot.title = element_text(face = "bold", color = "#333333"),
axis.title = element_text(color = "#333333"),
axis.text = element_text(color = "#333333")
) +
labs(
title = paste("Distribution of", var),
x = "Value",
y = "Count"
)
print(p)
}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(caret)
library(e1071)  #  SVM
library(rpart)  #  Decision Tree
library(randomForest)
library(pROC)   # ROC,AUC
library(rpart.plot)
library(tree)
library(ggplot2)
library(klaR)
data <- read.csv('diabetes_data.csv', header=TRUE)
colnames(data)
str(data)
#Check NA
sum(is.na(data))
# head(data,5)
# summary(data)
# Sample size by outcome
table(data$Diabetes)
# Or more descriptive
cat("Yes (Diabetes): n =", sum(data$Diabetes == 1), "\n")
cat("No (Diabetes): n =", sum(data$Diabetes == 0), "\n")
data$Diabetes <- factor(data$Diabetes,
levels= c(0,1),
labels =c("No","Yes"))
# data$Diabetes
tbl1 <- table(data$Diabetes)
tbl1
prop.table(table(data$Diabetes))
#Main numerical variables
numerical_var <- c("Age","BMI", "GenHlth", "MentHlth", "PhysHlth")
main <- data %>%
summarise(
across(
all_of(numerical_var),
list(
mean  = ~mean(., na.rm = TRUE),
median = ~median(., na.rm = TRUE),
sd     = ~sd(., na.rm = TRUE),
min    = ~min(., na.rm = TRUE),
max    = ~max(., na.rm = TRUE)
),
.names = "{.col}_{.fn}"
)
)
print("Main statistics for key numeric variables")
main
# data %>%
#  pivot_longer(all_of(numerical_var)) %>%
#  ggplot(aes(x = value)) +
#  geom_histogram(bins = 30, color = "white") +
#  facet_wrap(~ name, scales = "free") +
#  theme_minimal() +
#  labs(
#    title = "Distribution of numeric predictors",
#    x = "Value",
#    y = "Count"
#  )
# Loop through each numeric variable and plot individually
# Define custom colors for each variable
custom_colors <- c("Age" = "#1f77b4", "BMI" = "#ff7f0e", "GenHlth" = "#2ca02c",
"MentHlth" = "#d62728", "PhysHlth" = "#9467bd")
# Loop through each variable
for (var in numerical_var) {
# Create histogram data manually to find tallest bin
hist_data <- data %>%
select(all_of(var)) %>%
rename(value = all_of(var)) %>%
mutate(bin = cut(value, breaks = 30)) %>%
count(bin) %>%
mutate(bin_mid = as.numeric(sub("\\((.+),.*", "\\1", bin)) +
(as.numeric(sub(".*,(.+)\\]", "\\1", bin)) -
as.numeric(sub("\\((.+),.*", "\\1", bin))) / 2)
# Identify tallest bin
max_count <- max(hist_data$n)
# Plot
p <- ggplot(hist_data, aes(x = bin_mid, y = n)) +
geom_col(aes(fill = n == max_count), color = "black", width = 1) +
scale_fill_manual(values = c("TRUE" = "red", "FALSE" = custom_colors[[var]]), guide = "none") +
theme_minimal(base_size = 14) +
theme(
panel.grid = element_blank(),
axis.line = element_line(color = "black"),
axis.ticks = element_line(color = "black"),
plot.title = element_text(face = "bold", color = "#333333"),
axis.title = element_text(color = "#333333"),
axis.text = element_text(color = "#333333")
) +
labs(
title = paste("Distribution of", var),
x = "Value",
y = "Count"
)
print(p)
}
getwd)_
getwd()
knitr::opts_chunk$set(echo = TRUE)
clear
clc
